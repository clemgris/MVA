{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IAE8kMxe6E6k"
      },
      "source": [
        "# MVA - Homework 1 - Reinforcement Learning (2022/2023)\n",
        "\n",
        "**Name:** GRISLAIN ClÃ©mence"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vY4MH0nU637o"
      },
      "source": [
        "## Instructions\n",
        "\n",
        "* The deadline is **November 10 at 11:59 pm (Paris time).**\n",
        "\n",
        "* By doing this homework you agree to the late day policy, collaboration and misconduct rules reported on [Piazza](https://piazza.com/class/l4y5ubadwj64mb/post/6).\n",
        "\n",
        "* **Mysterious or unsupported answers will not receive full credit**. A correct answer, unsupported by calculations, explanation, or algebraic work will receive no credit; an incorrect answer supported by substantially correct calculations and explanations might still receive partial credit.\n",
        "\n",
        "* Answers should be provided in **English**."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YB__2uUC5U1r"
      },
      "source": [
        "# Colab setup"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2XNj1_VZ2FGJ"
      },
      "source": [
        "from IPython import get_ipython\n",
        "\n",
        "if 'google.colab' in str(get_ipython()):\n",
        "  # install rlberry library\n",
        "  !pip install git+https://github.com/rlberry-py/rlberry.git@mva2021#egg=rlberry[default] > /dev/null 2>&1\n",
        "\n",
        "  # install ffmpeg-python for saving videos\n",
        "  !pip install ffmpeg-python > /dev/null 2>&1\n",
        "\n",
        "  # packages required to show video\n",
        "  !pip install pyvirtualdisplay > /dev/null 2>&1\n",
        "  !apt-get install -y xvfb python-opengl ffmpeg > /dev/null 2>&1\n",
        "\n",
        "  print(\"Libraries installed, please restart the runtime!\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s8F7RiPXjutB"
      },
      "source": [
        "# Create directory for saving videos\n",
        "!mkdir videos > /dev/null 2>&1\n",
        "\n",
        "# Initialize display and import function to show videos\n",
        "import rlberry.colab_utils.display_setup\n",
        "from rlberry.colab_utils.display_setup import show_video"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KISV44N_nCNm"
      },
      "source": [
        "# Useful libraries\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm import tqdm, trange"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L4hKBRTCh6Gk"
      },
      "source": [
        "# Preparation\n",
        "\n",
        "In the coding exercises, you will use a *grid-world* MDP, which is represented in Python using the interface provided by the [Gym](https://gym.openai.com/) library. The cells below show how to interact with this MDP and how to visualize it.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "514mHDeQooKa"
      },
      "source": [
        "from rlberry.envs import GridWorld\n",
        "\n",
        "def get_env():\n",
        "  \"\"\"Creates an instance of a grid-world MDP.\"\"\"\n",
        "  env = GridWorld(\n",
        "      nrows=5,\n",
        "      ncols=7,\n",
        "      reward_at = {(0, 6):1.0},\n",
        "      walls=((0, 4), (1, 4), (2, 4), (3, 4)),\n",
        "      success_probability=0.9,\n",
        "      terminal_states=((0, 6),)\n",
        "  )\n",
        "  return env\n",
        "\n",
        "def render_policy(env, policy=None, horizon=50):\n",
        "  \"\"\"Visualize a policy in an environment\n",
        "\n",
        "  Args:\n",
        "    env: GridWorld\n",
        "        environment where to run the policy\n",
        "    policy: np.array\n",
        "        matrix mapping states to action (Ns).\n",
        "        If None, runs random policy.\n",
        "    horizon: int\n",
        "        maximum number of timesteps in the environment.\n",
        "  \"\"\"\n",
        "  env.enable_rendering()\n",
        "  state = env.reset()                       # get initial state\n",
        "  for timestep in range(horizon):\n",
        "      if policy is None:\n",
        "        action = env.action_space.sample()  # take random actions\n",
        "      else:\n",
        "        action = policy[state]\n",
        "      next_state, reward, is_terminal, info = env.step(action)\n",
        "      state = next_state\n",
        "      if is_terminal:\n",
        "        break\n",
        "  # save video and clear buffer\n",
        "  env.save_video('./videos/gw.mp4', framerate=5)\n",
        "  env.clear_render_buffer()\n",
        "  env.disable_rendering()\n",
        "  # show video\n",
        "  show_video('./videos/gw.mp4')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hQAHUBw_ifMI"
      },
      "source": [
        "# Create an environment and visualize it\n",
        "env = get_env()\n",
        "render_policy(env)  # visualize random policy\n",
        "\n",
        "# The reward function and transition probabilities can be accessed through\n",
        "# the R and P attributes:\n",
        "print(f\"Shape of the reward array = (S, A) = {env.R.shape}\")\n",
        "print(f\"Shape o f the transition array = (S, A, S) = {env.P.shape}\")\n",
        "print(f\"Reward at (s, a) = (1, 0): {env.R[1, 0]}\")\n",
        "print(f\"Prob[s\\'=2 | s=1, a=0]: {env.P[1, 0, 2]}\")\n",
        "print(f\"Number of states and actions: {env.Ns}, {env.Na}\")\n",
        "\n",
        "# The states in the griworld correspond to (row, col) coordinates.\n",
        "# The environment provides a mapping between (row, col) and the index of\n",
        "# each state:\n",
        "print(f\"Index of state (1, 0): {env.coord2index[(1, 0)]}\")\n",
        "print(f\"Coordinates of state 5: {env.index2coord[5]}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ibGD_3I89CNu"
      },
      "source": [
        "# Part 1 - Dynamic Programming"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NR7h5won9NQY"
      },
      "source": [
        "## Question 1.1\n",
        "\n",
        "Consider a general MDP with a discount factor of $\\gamma < 1$. Assume that the horizon is infinite (so there is no termination). A policy $\\pi$ in this MDP\n",
        "induces a value function $V^\\pi$. Suppose an affine transformation is applied to the reward, what is\n",
        "the new value function? Is the optimal policy preserved?\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "313W4K3B_LtN"
      },
      "source": [
        "### **Answer**\n",
        "\n",
        "$V^{\\pi}(s_t) = \\mathbb{E}(\\sum_{t=1}^{\\inf} \\gamma^t r(s_t, a_t) | s_0, a_t\\sim\\pi(s_t))$\n",
        "\n",
        "$ \\pi^* \\in \\underset{\\pi}{\\operatorname{argmax}} \\left( V^{\\pi} \\right) $\n",
        "\n",
        "\\\\\n",
        "affine transformation: $r = a\\times r + b, (a,b)\\in \\mathbb{R}^2$\n",
        "\n",
        "$V'^{\\pi}(s_t) = \\mathbb{E}\\left(\\sum_{t=1}^{\\inf} \\gamma^t (a \\times r(s_t, a_t) + b) | s_0, a_t\\sim\\pi(s_t)\\right)$\n",
        "$V'^{\\pi}(s_t) = \\mathbb{E}\\left(\\sum_{t=1}^{\\inf} \\gamma^t (a \\times r(s_t, a_t)) | s_0, a_t\\sim\\pi(s_t)\\right) + b \\times \\sum_{t=1}^{\\inf} \\gamma^t$\n",
        "$V'^{\\pi}(s_t) = a \\times\\mathbb{E}\\left(\\sum_{t=1}^{\\inf} \\gamma^t r(s_t, a_t) | s_0, a_t\\sim\\pi(s_t)\\right) + \\frac{b}{1-\\gamma} $\n",
        "$V'^{\\pi}(s_t) = a \\times V^{\\pi}(s_t)+\\frac{b}{1-\\gamma} $\n",
        "\n",
        "\n",
        "\\\\\n",
        "*   if $a=0$:\n",
        "\n",
        "$$\\pi'^* \\in \\underset{\\pi}{\\operatorname{argmax}} \\left( V'^{\\pi} \\right) = \\underset{\\pi}{\\operatorname{argmax}} \\left( \\frac{b}{1-\\gamma} \\right) = \\Pi $$\n",
        "The optimal policy is not preserved.\n",
        "*   if $a<0$:\n",
        "$$\\pi'^* \\in \\underset{\\pi}{\\operatorname{argmax}} \\left( V'^{\\pi} \\right) = \\underset{\\pi}{\\operatorname{argmax}} \\left( a \\times V^{\\pi}(s_t) + \\frac{b}{1-\\gamma}\\right) =  \\underset{\\pi}{\\operatorname{argmax}} \\left( a \\times V^{\\pi}(s_t) \\right) = \\underset{\\pi}{\\operatorname{argmin}} \\left( V^{\\pi} \\right)$$\n",
        "\n",
        "The optimal policy is not preserved.\n",
        "\n",
        "*   if $a>0$:\n",
        "$$\\pi'^* \\in \\underset{\\pi}{\\operatorname{argmax}} \\left( V'^{\\pi} \\right) = \\underset{\\pi}{\\operatorname{argmax}} \\left( a \\times V^{\\pi}(s_t) + \\frac{b}{1-\\gamma}\\right) =  \\underset{\\pi}{\\operatorname{argmax}} \\left( a \\times V^{\\pi}(s_t) \\right) = \\underset{\\pi}{\\operatorname{argmax}} \\left( V^{\\pi} \\right)$$\n",
        "\n",
        "The optimal policy is  preserved.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0uCVgkDo9vTM"
      },
      "source": [
        "## Question 1.2\n",
        "\n",
        "Consider an infinite-horizon $\\gamma$-discounted MDP. We denote by $Q^*$ the $Q$-function of the optimal policy $\\pi^*$. Prove that, for any function $Q(s, a)$ (which is **not** necessarily the value function of a policy), the following inequality holds for any state $s$:\n",
        "\n",
        "$$\n",
        "V^{\\pi_Q}(s) \\geq V^*(s) - \\frac{2}{1-\\gamma}||Q^*-Q||_\\infty,\n",
        "$$\n",
        "\n",
        "where $||Q^*-Q||_\\infty = \\max_{s, a} |Q^*(s, a) - Q(s, a)|$ and $\\pi_Q(s) \\in \\arg\\max_a Q(s, a)$. Can you use this result to show that any policy $\\pi$ such that $\\pi(s) \\in \\arg\\max_a Q^*(s, a)$ is optimal?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8MqGWPPD_OAI"
      },
      "source": [
        "### **Answer**\n",
        "$\\forall s\\in S$\n",
        "\n",
        "$V^{\\pi_Q}(s) = \\underset{a}{\\operatorname{max}}Q^{\\pi_Q}(s,a) = Q^{\\pi_Q}(s, \\pi_Q(s))$\n",
        "\n",
        "$V^*(s) = Q^*(s, \\pi^*(s))$\n",
        "\n",
        "\\\\\n",
        "Thus, $\\forall s \\in S$, \n",
        "\n",
        "$V^*(s) - V^{\\pi_Q}(s)$ \n",
        "\n",
        "$= Q(s, \\pi^*(s)) - Q^{\\pi_Q}(s, \\pi_Q(s))$\n",
        "\n",
        "$= Q^*(s, \\pi^*(s)) - Q^*(s, \\pi_Q(s)) + \\underbrace{Q^*(s, \\pi_Q(s)) - Q^{\\pi_Q}(s, \\pi_Q(s))}_{r(s,\\pi_Q(s)) + \\gamma \\underset{s' \\in S}{\\sum} p(s'|s, \\pi_Q(s)) V^*(s') - r(s,\\pi_Q(s)) - \\gamma \\underset{s' \\in S}{\\sum} p(s'|s, \\pi_Q(s)) V^{\\pi_Q}(s')}$\n",
        "\n",
        "$= Q^*(s, \\pi^*(s)) - Q^*(s, \\pi_Q(s)) + \\gamma \\underset{s' \\in S}{\\sum} p(s'|s, \\pi_Q(s)) \\left( V^*(s') - V^{\\pi_Q}(s')\\right)$\n",
        "\n",
        "$= Q^*(s, \\pi^*(s)) - \\underbrace{Q(s, \\pi_Q(s))}_{ = \\underset{a}{\\operatorname{max}}Q(s,a)  \\geq Q(s, \\pi^*(s))} + Q(s, \\pi_Q(s)) - Q^*(s, \\pi_Q(s)) + \\gamma \\underset{s' \\in S}{\\sum} p(s'|s, \\pi_Q(s)) \\left( V^*(s') - V^{\\pi_Q}(s')\\right)$\n",
        "\n",
        "$\\leq Q^*(s, \\pi^*(s)) - Q(s, \\pi^*(s)) + Q(s, \\pi_Q(s)) - Q^*(s, \\pi_Q(s)) + \\gamma \\underset{s' \\in S}{\\sum} p(s'|s, \\pi_Q(s)) \\left( V^*(s') - V^{\\pi_Q}(s')\\right)$\n",
        "\n",
        "$\\leq |Q^*(s, \\pi^*(s)) - Q(s, \\pi^*(s))| + |Q^*(s, \\pi_Q(s)) -Q(s, \\pi_Q(s)) | + \\gamma \\underset{s' \\in S}{\\sum} p(s'|s, \\pi_Q(s)) \\left( V^*(s') - V^{\\pi_Q}(s')\\right)$\n",
        "\n",
        "$\\leq 2 \\times ||Q^* - Q||_{\\infty} + \\gamma \\underset{s' \\in S}{\\sum} p(s'|s, \\pi_Q(s)) \\left( V^*(s') - V^{\\pi_Q}(s')\\right)$\n",
        "\n",
        "$\\leq 2 \\times ||Q^* - Q||_{\\infty} + \\gamma \\times ||V^* - V^{\\pi_Q}||_{\\infty}$\n",
        "\n",
        "\n",
        "hence we have, $\\forall s \\in S$\n",
        "\n",
        "$$V^*(s) - V^{\\pi_Q}(s) \\leq 2 \\times ||Q^* - Q||_{\\infty} + \\gamma \\times ||V^* - V_Q||_{\\infty}$$\n",
        "\n",
        "$$ \\implies ||V^* - V^{\\pi_Q}||_{\\infty} \\leq 2 \\times ||Q^* - Q||_{\\infty} + \\gamma \\times ||V^* - V_Q||_{\\infty}$$\n",
        "\n",
        "$$ \\implies ||V^* - V^{\\pi_Q}||_{\\infty} \\leq \\frac{2}{1-\\gamma} \\times ||Q^* - Q||_{\\infty}$$\n",
        "\n",
        "$$ \\implies V^*(s) - V^{\\pi_Q}(s) \\leq \\frac{2}{1-\\gamma} \\times ||Q^* - Q||_{\\infty}$$\n",
        "\n",
        "\\\\\n",
        "Let $\\pi_1$ be a policy st $\\forall s \\in S, \\pi_1(s) \\in \\arg\\max_a Q^*(s, a)$. According to the previous result, we have:\n",
        "$$ V^{\\pi_1}(s) \\geq V^*(s) - \\frac{2}{1-\\gamma}||Q^*-Q^*||_\\infty $$\n",
        "$$ \\implies V^{\\pi_1}(s) \\geq V^*(s)$$\n",
        "and by definition of $V^* = \\underset{\\pi}{\\text{max }}V^{\\pi}$\n",
        "$$\\implies V^{\\pi_1}(s) = V^*(s)$$\n",
        "$$\\implies \\pi_1 \\text{ is optimal}$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yIrtb7sihYcM"
      },
      "source": [
        "## Question 1.3\n",
        "\n",
        "In this question, you will implement and compare the policy and value iteration algorithms for a finite MDP. \n",
        "\n",
        "Complete the functions `policy_evaluation`, `policy_iteration` and `value_iteration` below.\n",
        "\n",
        "\n",
        "Compare value iteration and policy iteration. Highlight pros and cons of each method."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GLmQtk-wt0HS"
      },
      "source": [
        "### **Answer**\n",
        "\n",
        "Value iteration:\n",
        "\n",
        "\n",
        "*   **Pros:** for each iteration step, complexity in $O(S^2 \\times A)$.\n",
        "*   **Cons:** only asympotical convergence.\n",
        "\n",
        "Policy iteration:\n",
        "\n",
        "\n",
        "*   **Pros:** convergence in a finite number of iterations.\n",
        "*   **Cons:** for each iteration step, full policy iteration, complexity in $O\\left(S^2\\times \\frac{\\log(\\frac{1}{tol})}{\\log(\\frac{1}{\\gamma})}\\right)$ which might be expensive.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9yI0YYtMmpDQ"
      },
      "source": [
        "def policy_evaluation(P, R, policy, gamma=0.9, tol=1e-2):\n",
        "    \"\"\"\n",
        "    Args:\n",
        "        P: np.array\n",
        "            transition matrix (NsxNaxNs)\n",
        "        R: np.array\n",
        "            reward matrix (NsxNa)\n",
        "        policy: np.array\n",
        "            matrix mapping states to action (Ns)\n",
        "        gamma: float\n",
        "            discount factor\n",
        "        tol: float\n",
        "            precision of the solution\n",
        "    Return:\n",
        "        value_function: np.array\n",
        "            The value function of the given policy\n",
        "    \"\"\"\n",
        "    Ns, Na = R.shape\n",
        "    value_function = np.zeros(Ns)\n",
        "    # ====================================================\n",
        "    new_value_function = np.array([R[s, a] + gamma * (P[s, a, :] @ value_function) for s,a in enumerate(policy)])\n",
        "    while np.linalg.norm(new_value_function - value_function) > tol:\n",
        "      value_function = new_value_function\n",
        "      new_value_function = np.array([R[s, a] + gamma * (P[s, a, :] @ value_function) for s,a in enumerate(policy)])\n",
        "    value_function = new_value_function\n",
        "    # ====================================================\n",
        "    return value_function"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def policy_evaluation_matrix_inv(P, R, policy, gamma=0.9):\n",
        "    \"\"\"\n",
        "    Args:\n",
        "        P: np.array\n",
        "            transition matrix (NsxNaxNs)\n",
        "        R: np.array\n",
        "            reward matrix (NsxNa)\n",
        "        policy: np.array\n",
        "            matrix mapping states to action (Ns)\n",
        "        gamma: float\n",
        "            discount factor\n",
        "\n",
        "    Return:\n",
        "        value_function: np.array\n",
        "            The value function of the given policy using matrix inversion\n",
        "    \"\"\"\n",
        "    Ns, Na = R.shape\n",
        "    value_function = np.zeros(Ns)\n",
        "    R_pi = np.array([R[s, a] for s,a in enumerate(policy)])\n",
        "    P_pi = np.array([P[s, a, :] for s,a in enumerate(policy)])\n",
        "    value_function = np.linalg.inv((np.identity(Ns) - gamma * P_pi)) @ R_pi\n",
        "    return value_function"
      ],
      "metadata": {
        "id": "sezt0NegtIKq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ncqbPx99ncVY"
      },
      "source": [
        "def policy_iteration(P, R, gamma=0.9, tol=1e-3):\n",
        "    \"\"\"\n",
        "    Args:\n",
        "        P: np.array\n",
        "            transition matrix (NsxNaxNs)\n",
        "        R: np.array\n",
        "            reward matrix (NsxNa)\n",
        "        gamma: float\n",
        "            discount factor\n",
        "        tol: float\n",
        "            precision of the solution\n",
        "    Return:\n",
        "        policy: np.array\n",
        "            the final policy\n",
        "        V: np.array\n",
        "            the value function associated to the final policy\n",
        "    \"\"\"\n",
        "    Ns, Na = R.shape\n",
        "    V = np.zeros(Ns)\n",
        "    policy = np.ones(Ns, dtype=int)\n",
        "    # ====================================================\n",
        "    Vnew = policy_evaluation(P, R, policy, gamma=gamma, tol=tol)\n",
        "    while not np.array_equal(V, Vnew):\n",
        "      V = Vnew\n",
        "      policy = np.argmax(R + gamma * (P @ V), axis=1)\n",
        "      Vnew = policy_evaluation(P, R, policy, gamma=gamma, tol=tol)\n",
        "    # ====================================================\n",
        "    return policy, V"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3jB7dfA5nRCZ"
      },
      "source": [
        "def value_iteration(P, R, gamma=0.9, tol=1e-3):\n",
        "    \"\"\"\n",
        "    Args:\n",
        "        P: np.array\n",
        "            transition matrix (NsxNaxNs)\n",
        "        R: np.array\n",
        "            reward matrix (NsxNa)\n",
        "        gamma: float\n",
        "            discount factor\n",
        "        tol: float\n",
        "            precision of the solution\n",
        "    Return:\n",
        "        Q: final Q-function (at iteration n)\n",
        "        greedy_policy: greedy policy wrt Qn\n",
        "        Qfs: all Q-functions generated by the algorithm (for visualization)\n",
        "    \"\"\"\n",
        "    Ns, Na = R.shape\n",
        "    Q = np.zeros((Ns, Na))\n",
        "    Qfs = [Q]\n",
        "    # ====================================================\n",
        "    Qnew = R + gamma * (P @ np.max(Q, axis=1))\n",
        "    Qfs.append(Qnew)\n",
        "    while np.linalg.norm(Q - Qnew) > tol:\n",
        "      Q = Qnew\n",
        "      Qnew = R + gamma * (P @ np.max(Q, axis=1))\n",
        "      Qfs.append(Qnew)\n",
        "    Q = Qnew\n",
        "    greedy_policy = np.argmax(Q, axis=1)\n",
        "    # ====================================================\n",
        "    return Q, greedy_policy, Qfs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-Fi0IzZJp74Z"
      },
      "source": [
        "### Testing your code"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P7JKrc1oqFI2"
      },
      "source": [
        "# Parameters\n",
        "tol = 1e-5\n",
        "gamma = 0.99\n",
        "\n",
        "# Environment\n",
        "env = get_env()\n",
        "\n",
        "# run value iteration to obtain Q-values\n",
        "VI_Q, VI_greedypol, all_qfunctions = value_iteration(env.P, env.R, gamma=gamma, tol=tol)\n",
        "\n",
        "# render the policy\n",
        "print(\"[VI]Greedy policy: \")\n",
        "render_policy(env, VI_greedypol)\n",
        "\n",
        "# compute the value function of the greedy policy using matrix inversion\n",
        "# ====================================================\n",
        "greedy_V = policy_evaluation_matrix_inv(env.P, env.R, VI_greedypol, gamma=gamma)\n",
        "# ====================================================\n",
        "\n",
        "# show the error between the computed V-functions and the final V-function\n",
        "# (that should be the optimal one, if correctly implemented)\n",
        "# as a function of time\n",
        "final_V = all_qfunctions[-1].max(axis=1)\n",
        "norms = [ np.linalg.norm(q.max(axis=1) - final_V) for q in all_qfunctions]\n",
        "plt.plot(norms)\n",
        "plt.xlabel('Iteration')\n",
        "plt.ylabel('Error')\n",
        "plt.title(\"Value iteration: convergence\")\n",
        "\n",
        "#### POLICY ITERATION ####\n",
        "PI_policy, PI_V = policy_iteration(env.P, env.R, gamma=gamma, tol=tol)\n",
        "print(\"\\n[PI]final policy: \")\n",
        "render_policy(env, PI_policy)\n",
        "\n",
        "## Uncomment below to check that everything is correct\n",
        "assert np.allclose(PI_policy, VI_greedypol),\\\n",
        "    \"You should check the code, the greedy policy computed by VI is not equal to the solution of PI\"\n",
        "\n",
        "assert np.allclose(PI_V, greedy_V),\\\n",
        "     \"Since the policies are equal, even the value function should be\"\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2V1QdoH-xFX0"
      },
      "source": [
        "# Part 2 - Tabular RL"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qf51VhoPxbV4"
      },
      "source": [
        "## Question 2.1\n",
        "\n",
        "The code below collects two datasets of transitions (containing states, actions, rewards and next states) for a discrete MDP.\n",
        "\n",
        "For each of the datasets:\n",
        "\n",
        "1. Estimate the transitions and rewards, $\\hat{P}$ and $\\hat{R}$.\n",
        "2. Compute the optimal value function and the optimal policy with respect to the estimated MDP (defined by $\\hat{P}$ and $\\hat{R}$), which we denote by $\\hat{\\pi}$ and $\\hat{V}$.\n",
        "3. Numerically compare the performance of $\\hat{\\pi}$ and $\\pi^\\star$ (the true optimal policy), and the error between $\\hat{V}$ and $V^*$ (the true optimal value function).\n",
        "\n",
        "Which of the two data collection methods do you think is better? Why?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eWSyewG2EZpJ"
      },
      "source": [
        "### **Answer**\n",
        "\n",
        "The uniform dataset is clearly better than the one generated by random policies.\n",
        "\n",
        "In fact, from the plots below, we can see that for all size of samples, the error ($L_2$ norm and infinite norm) between the estimated and the true value function, is greater (or equal for very large dataset) for $\\hat{V}_{random}$ than for $\\hat{V}_{uniform}$. The same result is obtained for the error between the true and the estimated policies (error computed thanks to the $L_0$ norm).\n",
        "\n",
        "In the random policy dataset, as the policy is deterministic, with few iteration, some states (like the goal) won't be explored. This completelly bias the estimation of $\\hat{P}$ and $\\hat{R}$ (especially when the goal is not found by the policy). With a very large sample, however, the stochasticity of the next state for given (s,a) ensures the exploration and thus a good estimation of $\\hat{P}$ and $\\hat{R}$. In the other hand, the uniform dataset maximizes the exploration of the set of states which provides better $\\hat{P}$ and $\\hat{R}$ for the same size of sample."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8lNPhB28EcGd"
      },
      "source": [
        "def get_random_policy_dataset(env, n_samples):\n",
        "  \"\"\"Get a dataset following a random policy to collect data.\"\"\"\n",
        "  states = []\n",
        "  actions = []\n",
        "  rewards = []\n",
        "  next_states = []\n",
        "  \n",
        "  state = env.reset()\n",
        "  for _ in range(n_samples):\n",
        "    action = env.action_space.sample()\n",
        "    next_state, reward, is_terminal, info = env.step(action)\n",
        "    states.append(state)\n",
        "    actions.append(action)\n",
        "    rewards.append(reward)\n",
        "    next_states.append(next_state)\n",
        "    # update state\n",
        "    state = next_state\n",
        "    if is_terminal:\n",
        "      state = env.reset()\n",
        "\n",
        "  dataset = (states, actions, rewards, next_states)\n",
        "  return dataset\n",
        "\n",
        "def get_uniform_dataset(env, n_samples):\n",
        "  \"\"\"Get a dataset by uniformly sampling states and actions.\"\"\"\n",
        "  states = []\n",
        "  actions = []\n",
        "  rewards = []\n",
        "  next_states = []\n",
        "  for _ in range(n_samples):\n",
        "    state = env.observation_space.sample()\n",
        "    action = env.action_space.sample()\n",
        "    next_state, reward, is_terminal, info = env.sample(state, action)\n",
        "    states.append(state)\n",
        "    actions.append(action)\n",
        "    rewards.append(reward)\n",
        "    next_states.append(next_state)\n",
        "\n",
        "  dataset = (states, actions, rewards, next_states)\n",
        "  return dataset\n",
        "\n",
        "def estimate_MDP(dataset):\n",
        "  Phat = np.zeros((env.Ns, env.Na, env.Ns))\n",
        "  Rhat = np.zeros((env.Ns, env.Na))\n",
        "  N_as = np.zeros((env.Ns, env.Na))\n",
        "  (states, actions, rewards, next_states) = dataset\n",
        "  for (s, a, r, sp) in zip(states, actions, rewards, next_states):\n",
        "    Phat[s,a,sp] += 1\n",
        "    N_as[s,a] += 1\n",
        "    Rhat[s,a] += r\n",
        "  N_as = N_as * (N_as > 0) + np.ones(N_as.shape) * (N_as == 0)\n",
        "  Phat = Phat / N_as[:,:,None]\n",
        "  Rhat = Rhat / N_as\n",
        "\n",
        "  return Phat, Rhat\n",
        "\n",
        "# Collect two different datasets\n",
        "num_samples = 500\n",
        "env = get_env()\n",
        "dataset_1 = get_random_policy_dataset(env, num_samples)\n",
        "dataset_2 = get_uniform_dataset(env, num_samples)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Item 3: Estimate the MDP with the two datasets; compare the optimal value\n",
        "# functions in the true and in the estimated MDPs\n",
        "\n",
        "# Dataset_1\n",
        "Phat_1, Rhat_1 = estimate_MDP(dataset_1)\n",
        "policy_1, _ = policy_iteration(Phat_1, Rhat_1, gamma=gamma, tol=tol)\n",
        "print(\"[PI] Estimated MDP dataset_1: \")\n",
        "print()\n",
        "render_policy(env, policy_1)\n",
        "\n",
        "# Policy evaluation in the true env\n",
        "value_1 = policy_evaluation_matrix_inv(env.P, env.R, policy_1, gamma=gamma)\n",
        "print(\"Error ||V* - V_1||_inf:\", np.max(PI_V - value_1))\n",
        "print(\"Error ||V* - V_1||_2:\", np.linalg.norm(PI_V - value_1))\n",
        "print(\"Error 1 - ||pi* - pi_1||_0:\", np.sum(PI_policy == policy_1))\n",
        "\n",
        "#Dataset_2\n",
        "Phat_2, Rhat_2 = estimate_MDP(dataset_2)\n",
        "policy_2, value_2 = policy_iteration(Phat_2, Rhat_2, gamma=gamma, tol=tol)\n",
        "print(\"\\n[PI] Estimated MDP dataset_2: \")\n",
        "render_policy(env, policy_2)\n",
        "\n",
        "# Policy evaluation in the true env\n",
        "value_2 = policy_evaluation_matrix_inv(env.P, env.R, policy_2, gamma=gamma)\n",
        "print(\"Error ||V* - V_2||_inf:\", np.max(PI_V - value_2))\n",
        "print(\"Error ||V* - V_2||_2:\", np.linalg.norm(PI_V - value_2))\n",
        "print(\"Error 1 - ||pi* - pi_2||_0:\", np.sum(PI_policy == policy_2))"
      ],
      "metadata": {
        "id": "tFPj0QD5bESE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "N = 100\n",
        "\n",
        "X = []\n",
        "# Error on the value fonction (norm L2)\n",
        "Y1, Y2, yerr_1, yerr_2 = [], [], [], []\n",
        "# Error on the value function (norm inf)\n",
        "T1, T2, terr_1, terr_2 = [], [], [], []\n",
        "# Error on the policy (norm L0)\n",
        "Z1, Z2, zerr_1, zerr_2 = [], [], [], []\n",
        "\n",
        "for n in trange(500,5001,500):\n",
        "  X.append(n)\n",
        "  y1, y2, t1, t2, z1, z2 = [], [], [],[], [], []\n",
        "  for _ in range(N):\n",
        "    dataset_1 = get_random_policy_dataset(env, n)\n",
        "    Phat_1, Rhat_1 = estimate_MDP(dataset_1)\n",
        "    Q, policy_1, _= value_iteration(Phat_1, Rhat_1, gamma=gamma, tol=tol)\n",
        "    value_1 = np.max(Q, axis=1)\n",
        "    y1.append(np.linalg.norm(PI_V - value_1))\n",
        "    t1.append(np.max(np.abs(PI_V - value_1)))\n",
        "    z1.append(np.sum(PI_policy == policy_1))\n",
        "\n",
        "    dataset_2 = get_uniform_dataset(env, n)\n",
        "    Phat_2, Rhat_2 = estimate_MDP(dataset_2)\n",
        "    Q, policy_2, _ = value_iteration(Phat_2, Rhat_2, gamma=gamma, tol=tol)\n",
        "    value_2 = np.max(Q, axis=1)\n",
        "    y2.append(np.linalg.norm(PI_V - value_2))\n",
        "    t2.append(np.max(np.abs(PI_V - value_2)))\n",
        "    z2.append(np.sum(PI_policy == policy_2))\n",
        "  \n",
        "  Y1.append(np.mean(y1))\n",
        "  yerr_1.append(1.96 * np.std(y1)/np.sqrt(N))\n",
        "  Y2.append(np.mean(y2))\n",
        "  yerr_2.append(1.96 * np.std(y1)/np.sqrt(N))\n",
        "  T1.append(np.mean(t1))\n",
        "  terr_1.append(1.96 * np.std(t1)/np.sqrt(N))\n",
        "  T2.append(np.mean(t2))\n",
        "  terr_2.append(1.96 * np.std(t2)/np.sqrt(N))\n",
        "  Z1.append(np.mean(z1))\n",
        "  zerr_1.append(1.96 * np.std(z1)/np.sqrt(N))\n",
        "  Z2.append(np.mean(z2))\n",
        "  zerr_2.append(1.96 * np.std(z2)/np.sqrt(N))"
      ],
      "metadata": {
        "id": "uQtHUlIVs7h_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fig = plt.figure(figsize=(25, 5))\n",
        "\n",
        "fig.add_subplot(1,3,1)\n",
        "plt.errorbar(X, Y1, yerr=yerr_1, label=\"random policy\")\n",
        "plt.errorbar(X, Y2, yerr=yerr_2, label=\"uniform\")\n",
        "plt.title('Error on value function (L2 norm)')\n",
        "plt.legend()\n",
        "\n",
        "fig.add_subplot(1,3,2)\n",
        "plt.errorbar(X, T1, yerr=terr_1, label=\"random policy\")\n",
        "plt.errorbar(X, T2, yerr=terr_2, label=\"uniform\")\n",
        "plt.title('Error on value function (infinite norm)')\n",
        "plt.legend()\n",
        "\n",
        "fig.add_subplot(1,3,3)\n",
        "plt.errorbar(X, [1 - z for z in Z1], yerr=zerr_1, label=\"random policy\")\n",
        "plt.errorbar(X, [1 - z for z in Z2], yerr=zerr_2, label=\"uniform\")\n",
        "plt.title('Error on policy (using L0 norm)')\n",
        "plt.legend()\n"
      ],
      "metadata": {
        "id": "Q0DDmRBsvxCv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UKINsa_yGLGL"
      },
      "source": [
        "## Question 2.2\n",
        "\n",
        "Suppose that $\\hat{P}$ and $\\hat{R}$ are estimated from a dataset of exactly $N$ i.i.d. samples from **each** state-action pair. This means that, for each $(s,a)$, we have $N$ samples $\\{(s_1',r_1, \\dots, s_N', r_N\\}$, where $s_i' \\sim P(\\cdot | s,a)$ and $r_i \\sim R(s,a)$ for $i=1,\\dots,N$, and\n",
        "$$ \\hat{P}(s'|s,a) = \\frac{1}{N}\\sum_{i=1}^N \\mathbb{1}(s_i' = s'), $$\n",
        "$$ \\hat{R}(s,a) = \\frac{1}{N}\\sum_{i=1}^N r_i.$$\n",
        "Suppose that $R$ is a distribution with support in $[0,1]$. Let $\\hat{V}$ be the optimal value function computed in the empirical MDP (i.e., the one with transitions $\\hat{P}$ and rewards $\\hat{R}$). For any $\\delta\\in(0,1)$, derive an upper bound to the error\n",
        "\n",
        "$$ \\| \\hat{V} - V^* \\|_\\infty $$\n",
        "\n",
        "which holds with probability at least $1-\\delta$.\n",
        "\n",
        "**Note** Your bound should only depend on deterministic quantities like $N$, $\\gamma$, $\\delta$, $S$, $A$. It should *not* dependent on the actual random samples.\n",
        "\n",
        "**Hint** The following two inequalities may be helpful.\n",
        "\n",
        "1. **A (simplified) lemma**. For any state $\\bar{s}$,\n",
        "\n",
        "$$ |\\hat{V}(\\bar{s}) - V^*(\\bar{s})| \\leq \\frac{1}{1-\\gamma}\\max_{s,a} \\left| R(s,a) - \\hat{R}(s,a) + \\gamma \\sum_{s'}(P(s'|s,a) - \\hat{P}(s'|s,a)) V^*(s') \\right|$$\n",
        "\n",
        "2. **Hoeffding's inequality**. Let $X_1, \\dots X_N$ be $N$ i.i.d. random variables bounded in the interval $[0,b]$ for some $b>0$. Let $\\bar{X} = \\frac{1}{N}\\sum_{i=1}^N X_i$ be the empirical mean. Then, for any $\\epsilon > 0$,\n",
        "\n",
        "$$ \\mathbb{P}(|\\bar{X} - \\mathbb{E}[\\bar{X}]| > \\epsilon) \\leq 2e^{-\\frac{2N\\epsilon^2}{b^2}}.$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fKmdulLaMoiN"
      },
      "source": [
        "### **Answer**\n",
        "$\\forall \\bar{s} \\in S$\n",
        "\n",
        "$|\\hat{V}(\\bar{s}) - V^*(\\bar{s})| \\leq \\frac{1}{1-\\gamma}\\max_{s,a} \\left| R(s,a) - \\hat{R}(s,a) + \\gamma \\sum_{s'}(P(s'|s,a) - \\hat{P}(s'|s,a)) V^*(s') \\right|$\n",
        "$\\implies \\| \\hat{V} - V^* \\|_\\infty \\leq \\frac{1}{1-\\gamma}\\max_{s,a} \\left| R(s,a) - \\hat{R}(s,a) + \\gamma \\sum_{s'}(P(s'|s,a) - \\hat{P}(s'|s,a)) V^*(s') \\right|$\n",
        "\n",
        "\\\\\n",
        "Lets define: \\\\\n",
        "$\\overline{X}(a,s) = \\hat{R}(s,a) + \\gamma \\sum_{s'}\\hat{P}(s'|s,a) V^*(s')$\n",
        "\n",
        "$\\mathbb{E}\\left(\\overline{X}(a,s)\\right) = R(s,a) + \\gamma \\sum_{s'}P(s'|s,a) V^*(s')$\n",
        "\n",
        "\\\\\n",
        "And we have: \\\\\n",
        "$V^*(s') = \\mathbb{E}\\left( \\overset{\\infty}{\\underset{t=0}{\\sum}} \\gamma^t R(s_t, a_t)\\right) \\leq \\frac{R_{max}}{1-\\gamma} = \\frac{1}{1-\\gamma}$\n",
        "\n",
        "$\\implies$ $\\forall (a,s)\\in A\\times S$, $ \\overline{X}(a,s) = \\hat{R}(s,a) + \\gamma \\sum_{s'}\\hat{P}(s'|s,a) V^*(s') \\leq 1 + \\frac{\\gamma}{1-\\gamma} = \\frac{1}{1-\\gamma}$\n",
        "\n",
        "\\\\\n",
        "$$\\mathbb{P}\\left( \\max_{s,a} \\left| R(s,a) - \\hat{R}(s,a) + \\gamma \\sum_{s'}(P(s'|s,a) - \\hat{P}(s'|s,a)) V^*(s') \\right| < \\epsilon \\right)$$\n",
        "$$ = \\mathbb{P}\\left( \\underset{a,s}{\\bigcap} \\left| R(s,a) - \\hat{R}(s,a) + \\gamma \\sum_{s'}(P(s'|s,a) - \\hat{P}(s'|s,a)) V^*(s') \\right| < \\epsilon \\right)$$\n",
        "\n",
        "by independance\n",
        "$$ = \\underset{a,s}{\\Pi} \\left[\\mathbb{P}\\left(\\left| R(s,a) - \\hat{R}(s,a) + \\gamma \\sum_{s'}(P(s'|s,a) - \\hat{P}(s'|s,a)) V^*(s') \\right| < \\epsilon \\right)\\right]$$\n",
        "$$= \\underset{a,s}{\\Pi} \\left[\\mathbb{P}\\left(\\left|\\overline{X}(a,s) -   \\mathbb{E}\\left(\\overline{X}(a,s)\\right)\\right| < \\epsilon \\right)\\right]$$\n",
        "\n",
        "by applying Holder inequality on each $\\overline{X}(a,s)$ we obtain\n",
        "\n",
        "$$\\mathbb{P}\\left( \\max_{s,a} \\left| R(s,a) - \\hat{R}(s,a) + \\gamma \\sum_{s'}(P(s'|s,a) - \\hat{P}(s'|s,a)) V^*(s') \\right| < \\epsilon \\right) \\geq \\left(1 - 2e^{-\\frac{2N\\epsilon^2}{b^2}}\\right)^{A\\times S}$$\n",
        "\n",
        "\\\\\n",
        "We want $$ \\left(1 - 2e^{-\\frac{2N\\epsilon^2}{b^2}}\\right)^{A\\times S} = 1-\\delta \\implies \\epsilon = \\frac{b}{\\sqrt{2N}}\\sqrt{log\\left( \\frac{1 - (1-\\delta)^{SA}}{2}\\right)}$$ with $b=\\frac{1}{1-\\gamma}$ upper bound on $\\overline{X}(a,s)$ $\\forall (a,s)\\in A\\times S$.\n",
        "\n",
        "\\\\\n",
        "Hence we have: \\\\\n",
        "$$ \\mathbb{P}\\left( \\| \\hat{V} - V^* \\|_\\infty  <  \\frac{1}{(1-\\gamma)^2\\sqrt{2N}}\\sqrt{log\\left( \\frac{1 - (1-\\delta)^{SA}}{2}\\right)}\\right)  \\geq \\mathbb{P}\\left( \\frac{1}{1-\\gamma}\\max_{s,a} \\left| R(s,a) - \\hat{R}(s,a) + \\gamma \\sum_{s'}(P(s'|s,a) - \\hat{P}(s'|s,a)) V^*(s') \\right| < \\frac{\\epsilon}{1 - \\gamma} \\right) \\geq 1-\\delta$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tpqwCBG2MwxO"
      },
      "source": [
        "## Question 2.3\n",
        "\n",
        "Suppose once again that we are given a dataset of $N$ samples in the form of tuples $(s_i,a_i,s_i',r_i)$. We know that each tuple contains a valid transition from the true MDP, i.e., $s_i' \\sim P(\\cdot | s_i, a_i)$ and $r_i \\sim R(s_i,a_i)$, while the state-action pairs $(s_i,a_i)$ from which the transition started can be arbitrary.\n",
        "\n",
        "Suppose we want to apply Q-learning to this MDP. Can you think of a way to leverage this offline data to improve the sample-efficiency of the algorithm? What if we were using SARSA instead?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mbYTKetHOYU_"
      },
      "source": [
        "### **Answer**\n",
        "\n",
        "\n",
        "\n",
        "1.   One way to use this dataset is to compute a good estimation of teh value function with linear fitted Q-itteration, and then to start from this estimation to apply Q-learning and from the greaddy policy of this estimated value function to start SARSA.\n",
        "2.   Another way could be to use the dataset to have a better estimation of the Bellman error. In fact, for a given $(s_t, a_t, r_t, s_{t+1})$ instead of computed the Bellman error with $r_t$ we could use the mean reward found on the dataset $\\bar{r}=\\frac{1}{N(s,a)}\\underset{(s,a,r,s')\\in dataset}{\\sum}r\\times 1_{\\{(s,a)=(s_t,a_t)\\}}$. In a way, we try to estimate the reward distribution $\\hat{R}$ like in the tabular Dyna-Q algorithm. For SARSA, as the Bellman error is also computed, we could do the same.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "542QxKsSOs21"
      },
      "source": [
        "# Part 3 - RL with Function Approximation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OiGZBiJ4PiIE"
      },
      "source": [
        "## Question 3.1\n",
        "\n",
        "Given a datset $(s_i, a_i, r_i, s_i')$ of (states, actions, rewards, next states), the Fitted Q-Iteration (FQI) algorithm proceeds as follows:\n",
        "\n",
        "\n",
        "* We start from a $Q$ function $Q_0 \\in \\mathcal{F}$, where $\\mathcal{F}$ is a function space;\n",
        "* At every iteration $k$, we compute $Q_{k+1}$ as:\n",
        "\n",
        "$$\n",
        "Q_{k+1}\\in\\arg\\min_{f\\in\\mathcal{F}} \\frac{1}{2}\\sum_{i=1}^N\n",
        "\\left(\n",
        "  f(s_i, a_i) - y_i^k\n",
        "\\right)^2 + \\lambda \\Omega(f)\n",
        "$$\n",
        "where $y_i^k = r_i + \\gamma \\max_{a'}Q_k(s_i', a')$, $\\Omega(f)$ is a regularization term and $\\lambda > 0$ is the regularization coefficient.\n",
        "\n",
        "\n",
        "Consider FQI with *linear* function approximation. That is, for a given feature map $\\phi : S \\rightarrow \\mathbb{R}^d$, we consider a parametric family of $Q$ functions $Q_\\theta(s,a) = \\phi(s)^T\\theta_a$ for $\\theta_a\\in\\mathbb{R}^d$. Suppose we are applying FQI on a given dataset of $N$ tuples of the form $(s_i, a_i, r_i, s_i')$ and we are at the $k$-th iteration. Let $\\theta_k \\in\\mathbb{R}^{d \\times A}$ be our current parameter. Derive the *closed-form* update to find $\\theta_{k+1}$, using $\\frac{1}{2}\\sum_a ||\\theta_a||_2^2$ as regularization."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7jx7aE41DkEM"
      },
      "source": [
        "### **Answer**\n",
        "\n",
        "$Q_{k+1} = Q_{\\theta_{k+1}} \\in\\arg\\min_{\\theta\\in\\mathcal{\\mathbb{R}^d}} \\frac{1}{2}\\sum_{i=1}^N\n",
        "\\left(\n",
        "  \\phi(s_i)^T\\theta_{a_i} - y_i^k\n",
        "\\right)^2 +  \\frac{\\lambda}{2}\\sum_a ||\\theta_a||_2^2$\n",
        "\n",
        "\\\\\n",
        "$\\theta^{k+1}$ is a critic point of the function:\n",
        "$F : \\theta  \t\\longrightarrow \\frac{1}{2}\\sum_{i=1}^N\n",
        "\\left(\n",
        "  \\phi(s_i)^T\\theta_{a_i} - y_i^k\n",
        "\\right)^2 +  \\frac{\\lambda}{2}\\sum_a ||\\theta_a||_2^2$\n",
        "\n",
        "\n",
        "Thus, $\\forall a \\in A,$ $\\theta^{k+1}_a$ is a critic point of the function \n",
        "$F_a : \\theta_a  \t\\longrightarrow \\frac{1}{2}\\sum_{i=1}^N\n",
        "\\left(\n",
        "  \\phi(s_i)^T\\theta_{a_i} - y_i^k\n",
        "\\right)^2 +  \\frac{\\lambda}{2}\\sum_a ||\\theta_a||_2^2$\n",
        "\n",
        "\\\\\n",
        "Hence $\\forall a \\in A,$ $\\frac{\\partial F_a}{\\partial a}(\\theta^{k+1}_a)=0$\n",
        "\n",
        "$\\implies \\sum_{i=1}^N\n",
        "\\left(\n",
        "  \\underbrace{\\phi(s_i)^T\\theta^{k+1}_{a_i}}_{\\in \\mathbb{R}} - y_i^k\n",
        "\\right)\\phi(s_i)\\mathbb{1}_{(a_i=a)} +  \\lambda\\theta^{k+1}_a = 0$\n",
        "\n",
        "$\\implies \\sum_{i=1}^N\n",
        "  \\phi(s_i)\\phi(s_i)^T\\theta^{k+1}_{a_i}\\mathbb{1}_{(a_i=a)} +  \\lambda\\theta^{k+1}_a = \\sum_{i=1}^N y_i^k\\phi(s_i)\\mathbb{1}_{(a_i=a)}$\n",
        "\n",
        "$\\implies \\left(\\sum_{i=1}^N\n",
        "  \\phi(s_i)\\phi(s_i)^T\\mathbb{1}_{(a_i=a)} +  \\lambda I_d\\right)\\theta^{k+1}_a = \\sum_{i=1}^N y_i^k\\phi(s_i)\\mathbb{1}_{(a_i=a)}$\n",
        "\n",
        "$\\implies \\theta^{k+1}_a = \\left(\\sum_{i=1}^N\n",
        "  \\phi(s_i)\\phi(s_i)^T\\mathbb{1}_{(a_i=a)} +  \\lambda I_d\\right)^{-1}\\sum_{i=1}^N y_i^k\\phi(s_i)\\mathbb{1}_{(a_i=a)}$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ewHzjm7MVGBg"
      },
      "source": [
        "## Question 3.2\n",
        "\n",
        "The code below creates a larger gridworld (with more states than the one used in the previous questions), and defines a feature map. Implement linear FQI to this environment (in the function `linear_fqi()` below), and compare the approximated $Q$ function to the optimal $Q$ function computed with value iteration.\n",
        "\n",
        "Can you improve the feature map in order to reduce the approximation error?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tu4g-HSnEcBs"
      },
      "source": [
        "### **Answer**\n",
        "\n",
        "With the defaults parameters, the value function map was coherent but the policy does not reach the goal. \n",
        "Different ideas have been tested in order to reduce the approximation error:\n",
        "\n",
        "1)   Feature map:\n",
        "*   Increase the number of parameters of the linear_fqi approximation (increase parameter dim)\n",
        "*   Change the norm used to compute the similarity matrix. The default norm is the $L_2$, but other noms can be more relevant (norm $L_1$ of inifite).\n",
        "*   Change the sigma used to compute the siminarity matrix.\n",
        "\n",
        "2)  Training parameters:\n",
        "\n",
        "*   The number of iteration. Too few iteration leads to poor generalization but too many to overfitting.\n",
        "*   The number of samples. The greater the number the samples, the better the information on the environnement, but this increases the training time. A trade-off need to be found.\n",
        "*   The parameter of the regularization $\\lambda$\n",
        "\n",
        "\n",
        "As a conclusion:\n",
        "\n",
        "*  The linear model with dim=15 was to small to fit the true value function, thus we increased the dimension of the feature map up to 50 which converges in a few number of iterations $\\approx 100$. (Smaller feature maps (size $\\approx 35$) also converges but for higher number of iterations $\\approx 1000$).\n",
        "\n",
        "*  The number of samples was also too small to provide good fitting. We increased it up to 10 000.\n",
        "\n",
        "*  Among the tree norms ($L_2$, $L_1$, $inf$), unsing the $L_1$ and the $infinite$ norms to compute the feature map leads to an estimated value function with high values in the areas close to the goal but on the other side of the obstacle. Trying to reach those values, the agent gets stuck behind the obstacle.This problem is less visible on the estimated value function computed with the $L_2$ norm. The agent reachs the goal. Thus, we can that say using $L2$ it provides the best estimation.\n",
        "\n",
        "However, in comparison with the true value function, we see that our model only concentrates high value around the goal whereas in the true value function, the high values are also \"diffused\" on horizontal path leading to the passage between the two obstacles, and on the full area behind the obstacles.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZovF3VXOVfCs"
      },
      "source": [
        "def get_large_gridworld():\n",
        "  \"\"\"Creates an instance of a grid-world MDP with more states.\"\"\"\n",
        "  walls = [(ii, 10) for ii in range(15) if (ii != 7 and ii != 8)]\n",
        "  env = GridWorld(\n",
        "      nrows=15,\n",
        "      ncols=15,\n",
        "      reward_at = {(14, 14):1.0},\n",
        "      walls=tuple(walls),\n",
        "      success_probability=0.9,\n",
        "      terminal_states=((14, 14),)\n",
        "  )\n",
        "  return env\n",
        "\n",
        "class GridWorldFeatureMap:\n",
        "  \"\"\"Create features for state-action pairs\n",
        "  \n",
        "  Args:\n",
        "    dim: int\n",
        "      Feature dimension\n",
        "    sigma: float\n",
        "      RBF kernel bandwidth\n",
        "  \"\"\"\n",
        "  def __init__(self, env, dim=15, sigma=0.25, norm='L2'):\n",
        "    self.index2coord = env.index2coord\n",
        "    self.n_states = env.Ns\n",
        "    self.n_actions = env.Na\n",
        "    self.dim = dim\n",
        "    self.sigma = sigma\n",
        "\n",
        "    n_rows = env.nrows\n",
        "    n_cols = env.ncols\n",
        "\n",
        "    # build similarity matrix\n",
        "    sim_matrix = np.zeros((self.n_states, self.n_states))\n",
        "    for ii in range(self.n_states):\n",
        "        row_ii, col_ii = self.index2coord[ii]\n",
        "        x_ii = row_ii / n_rows\n",
        "        y_ii = col_ii / n_cols\n",
        "        for jj in range(self.n_states):\n",
        "            row_jj, col_jj = self.index2coord[jj]\n",
        "            x_jj = row_jj / n_rows\n",
        "            y_jj = col_jj / n_cols\n",
        "            if norm=='L2':\n",
        "              dist = np.sqrt((x_jj - x_ii) ** 2.0 + (y_jj - y_ii) ** 2.0)\n",
        "            elif norm=='inf':\n",
        "              dist = max((x_jj - x_ii), (y_jj - y_ii))\n",
        "            elif norm=='L1':\n",
        "              dist = abs((x_jj - x_ii) + (y_jj - y_ii))\n",
        "            else:\n",
        "              assert False, \"Wrong norm name\"\n",
        "            sim_matrix[ii, jj] = np.exp(-(dist / sigma) ** 2.0)\n",
        "\n",
        "    # factorize similarity matrix to obtain features\n",
        "    uu, ss, vh = np.linalg.svd(sim_matrix, hermitian=True)\n",
        "    self.feats = vh[:dim, :]\n",
        "\n",
        "  def map(self, observation):\n",
        "    feat = self.feats[:, observation].copy()\n",
        "    return feat"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "InCfu7F9-TbS"
      },
      "source": [
        "env = get_large_gridworld()\n",
        "feat_map = GridWorldFeatureMap(env)\n",
        "\n",
        "# Visualize large gridworld\n",
        "render_policy(env)\n",
        "\n",
        "# The features have dimension (feature_dim).\n",
        "feature_example = feat_map.map(1) # feature representation of s=1\n",
        "print(feature_example)\n",
        "\n",
        "# Initial vector theta representing the Q function\n",
        "theta = np.zeros((feat_map.dim, env.action_space.n))\n",
        "print(theta.shape)\n",
        "print(feature_example @ theta) # approximation of Q(s=1, a)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p21KMmruugO1"
      },
      "source": [
        "def linear_fqi(env, feat_map, num_iterations, lambd=0.1, gamma=0.95):\n",
        "  \"\"\"\n",
        "  # Linear FQI implementation\n",
        "  # TO BE COMPLETED\n",
        "  \"\"\"\n",
        "  N = 10000\n",
        "  dataset = get_uniform_dataset(env, n_samples=N)\n",
        "  (states, actions, rewards, next_states) = dataset\n",
        "  theta = np.zeros((feat_map.dim, env.Na))\n",
        "\n",
        "  for it in trange(num_iterations):\n",
        "    A = np.zeros((feat_map.dim, feat_map.dim, env.Na))\n",
        "    b = np.zeros((feat_map.dim, env.Na))\n",
        "    for (s, a, r, sp) in zip(states, actions, rewards, next_states):\n",
        "      phi_s = feat_map.map(s).reshape((feat_map.dim, 1))\n",
        "      phi_sp = feat_map.map(sp).reshape((feat_map.dim, 1))\n",
        "      # print(feat.shape)\n",
        "      A[:,:,a] += phi_s @ np.transpose(phi_s)\n",
        "      temp =  (r + gamma * np.max(np.transpose(phi_sp) @ theta, axis=1)[0]) * phi_s\n",
        "      b[:,a] += temp[:,0]\n",
        "    for a in range(env.Na):\n",
        "      theta[:,a] = np.linalg.solve(A[:,:,a] + lambd * np.identity(feat_map.dim), b[:,a])\n",
        "  return theta\n",
        "\n",
        "# ----------------------------\n",
        "# Environment and feature map\n",
        "# ----------------------------\n",
        "env = get_large_gridworld()\n",
        "# you can change the parameters of the feature map, and even try other maps!\n",
        "norm = 'L1'\n",
        "feat_map = GridWorldFeatureMap(env, dim=50, sigma=0.30, norm=norm)\n",
        "\n",
        "# -------\n",
        "# Run FQI\n",
        "# -------\n",
        "theta = linear_fqi(env, feat_map, num_iterations=100)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Compute and run greedy policy\n",
        "Q_fqi = np.zeros((env.Ns, env.Na))\n",
        "for ss in range(env.Ns):\n",
        "  state_feat = feat_map.map(ss)\n",
        "  Q_fqi[ss, :] = state_feat @ theta\n",
        "\n",
        "V_fqi = Q_fqi.max(axis=1)\n",
        "policy_fqi = Q_fqi.argmax(axis=1)\n",
        "render_policy(env, policy_fqi, horizon=100)\n",
        "\n",
        "# Visualize the approximate value function in the gridworld.\n",
        "img = env.get_layout_img(V_fqi)    \n",
        "plt.imshow(img)\n",
        "plt.title(\"Estimated value function (feature map with norm {})\".format(norm))\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "nwAkvOLP8-O6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Compute and run greedy policy\n",
        "Q_fqi = np.zeros((env.Ns, env.Na))\n",
        "for ss in range(env.Ns):\n",
        "  state_feat = feat_map.map(ss)\n",
        "  Q_fqi[ss, :] = state_feat @ theta\n",
        "\n",
        "V_fqi = Q_fqi.max(axis=1)\n",
        "policy_fqi = Q_fqi.argmax(axis=1)\n",
        "render_policy(env, policy_fqi, horizon=100)\n",
        "\n",
        "# Visualize the approximate value function in the gridworld.\n",
        "img = env.get_layout_img(V_fqi)    \n",
        "plt.imshow(img)\n",
        "plt.title(\"Estimated value function (feature map with norm {})\".format(norm))\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "fktc2cDMhW4n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Compute and run greedy policy\n",
        "Q_fqi = np.zeros((env.Ns, env.Na))\n",
        "for ss in range(env.Ns):\n",
        "  state_feat = feat_map.map(ss)\n",
        "  Q_fqi[ss, :] = state_feat @ theta\n",
        "\n",
        "V_fqi = Q_fqi.max(axis=1)\n",
        "policy_fqi = Q_fqi.argmax(axis=1)\n",
        "render_policy(env, policy_fqi, horizon=100)\n",
        "\n",
        "# Visualize the approximate value function in the gridworld.\n",
        "img = env.get_layout_img(V_fqi)    \n",
        "plt.imshow(img)\n",
        "plt.title(\"Estimated value function (feature map with norm {})\".format(norm))\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "Yz5Pg-Vkh_WO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# True value function\n",
        "tol = 1e-5\n",
        "gamma=0.95\n",
        "true_Q, _, _ = value_iteration(env.P, env.R, gamma=gamma, tol=tol)\n",
        "true_V = true_Q.max(axis=1)\n",
        "\n",
        "# Visualize the true value function in the gridworld.\n",
        "img = env.get_layout_img(true_V)    \n",
        "plt.imshow(img)\n",
        "plt.title(\"True value function\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "gdEhujttAKMI"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}